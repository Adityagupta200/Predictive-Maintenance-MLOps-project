name: Notebook Smoke

on:
  pull_request:
  push:
    branches: [main, master]
  workflow_dispatch:

jobs:
  notebook-smoke:
    runs-on: ubuntu-latest
    env:
      # Use sqlite backend to avoid the file-store deprecation warning.
      # Artifacts will still be stored locally under MLFLOW_ARTIFACT_ROOT.
      MLFLOW_TRACKING_URI: sqlite:///${{ github.workspace }}/mlflow.db
      MLFLOW_EXPERIMENT_NAME: predictive-maintenance-cmapss
      MLFLOW_MODEL_NAME: model
      MLFLOW_ARTIFACT_ROOT: ${{ github.workspace }}/mlflow-artifacts

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps (incl. Jupyter + MLflow helpers)
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          make setup
          python -m pip install nbconvert jupyter ipykernel mlflow joblib

      - name: Clean MLflow state (CI only)
        run: |
          set -euo pipefail
          rm -f mlflow.db || true
          rm -rf "$MLFLOW_ARTIFACT_ROOT" || true
          mkdir -p "$MLFLOW_ARTIFACT_ROOT"

      - name: Prepare notebook inputs (generate artifacts)
        run: |
          set -euo pipefail
          mkdir -p artifacts/processed models

          # Generate required local inputs for Notebook 01
          if [ ! -f artifacts/processed/train.csv ] || [ ! -f artifacts/processed/val.csv ]; then
            if make -n preprocess >/dev/null 2>&1; then
              make preprocess
            elif make -n pipeline >/dev/null 2>&1; then
              make pipeline
            fi
          fi

          if [ ! -f models/best_model.joblib ]; then
            if make -n train >/dev/null 2>&1; then
              make train
            fi
          fi

          # Hard fail with clear message if still missing
          test -f artifacts/processed/train.csv || (echo "Missing artifacts/processed/train.csv" && exit 1)
          test -f artifacts/processed/val.csv || (echo "Missing artifacts/processed/val.csv" && exit 1)
          test -f models/best_model.joblib || (echo "Missing models/best_model.joblib" && exit 1)

          # Export explicit paths for notebooks
          echo "TRAIN_CSV=${GITHUB_WORKSPACE}/artifacts/processed/train.csv" >> "$GITHUB_ENV"
          echo "VAL_CSV=${GITHUB_WORKSPACE}/artifacts/processed/val.csv" >> "$GITHUB_ENV"
          echo "MODEL_PATH=${GITHUB_WORKSPACE}/models/best_model.joblib" >> "$GITHUB_ENV"

      - name: Execute notebook 01 (logs MLflow runs)
        run: |
          set -euo pipefail
          mkdir -p notebooks/_executed
          jupyter nbconvert --execute --to notebook \
            --ExecutePreprocessor.timeout=1800 \
            --output-dir notebooks/_executed \
            notebooks/01_mlflow_experiments.ipynb

      - name: Ensure experiment exists with artifact root
        run: |
          set -euo pipefail
          python - << 'PY'
          import os
          from pathlib import Path
          import mlflow

          mlflow.set_tracking_uri(os.environ["MLFLOW_TRACKING_URI"])

          name = os.environ["MLFLOW_EXPERIMENT_NAME"]
          artifact_root = Path(os.environ["MLFLOW_ARTIFACT_ROOT"]).resolve()

          exp = mlflow.get_experiment_by_name(name)
          if exp is None:
              exp_id = mlflow.create_experiment(name, artifact_location=str(artifact_root))
              print(f"Created experiment: {name} ({exp_id}) with artifact_location={artifact_root}")
          else:
              print(f"Experiment exists: {name} ({exp.experiment_id}) artifact_location={exp.artifact_location}")
          PY

      - name: Ensure at least one run has a logged model (fallback if Notebook 01 didn't)
        run: |
          set -euo pipefail
          python - << 'PY'
          import os
          from pathlib import Path
          from urllib.parse import urlparse

          import joblib
          import mlflow
          import mlflow.sklearn
          from mlflow.tracking import MlflowClient

          def run_has_model_artifact(client: MlflowClient, run_id: str, model_name: str) -> bool:
              # List root-level artifacts and check presence of "model_name"
              artifacts = client.list_artifacts(run_id, path="")
              return any(a.path == model_name for a in artifacts)

          mlflow.set_tracking_uri(os.environ["MLFLOW_TRACKING_URI"])
          mlflow.set_experiment(os.environ["MLFLOW_EXPERIMENT_NAME"])

          model_name = os.environ["MLFLOW_MODEL_NAME"]
          client = MlflowClient()

          exp = mlflow.get_experiment_by_name(os.environ["MLFLOW_EXPERIMENT_NAME"])
          if exp is None:
              raise SystemExit("Experiment not found (should have been created in previous step).")

          # Check if any existing ACTIVE run already has the model artifact
          runs = client.search_runs(
              experiment_ids=[exp.experiment_id],
              filter_string="attributes.lifecycle_stage = 'active'",
              order_by=["attributes.start_time DESC"],
              max_results=200,
          )
          if any(run_has_model_artifact(client, r.info.run_id, model_name) for r in runs):
              print("At least one run already has the model artifact; no fallback needed.")
              raise SystemExit(0)

          # Otherwise create a fallback run that logs the model
          model_path = Path(os.environ["MODEL_PATH"])
          if not model_path.exists():
              raise SystemExit(f"MODEL_PATH does not exist: {model_path}")

          model = joblib.load(model_path)

          with mlflow.start_run(run_name="ci-fallback-model") as run:
              mlflow.log_param("fallback_reason", "notebook_01_did_not_log_model")

              # Prefer the new API (name=) to avoid artifact_path deprecation warnings.
              try:
                  mlflow.sklearn.log_model(model, name=model_name)
              except TypeError:
                  # Backward compatibility if an older MLflow is used.
                  mlflow.sklearn.log_model(model, artifact_path=model_name)

              mlflow.log_metric("ci_fallback", 1.0)

              run_id = run.info.run_id

          # Verify it actually exists via the tracking API
          if not run_has_model_artifact(client, run_id, model_name):
              # Print artifact URI for debugging
              r = client.get_run(run_id)
              print(f"Fallback run artifact_uri={r.info.artifact_uri}")
              raise SystemExit(f"Fallback run did not produce '{model_name}' artifact as expected.")

          print(f"Created fallback run with model: {run_id}")
          PY

      - name: Prune runs missing model artifact (API-based; backend-agnostic)
        run: |
          set -euo pipefail
          python - << 'PY'
          import os
          import shutil
          from pathlib import Path
          from urllib.parse import urlparse, unquote

          import mlflow
          from mlflow.tracking import MlflowClient

          def run_has_model_artifact(client: MlflowClient, run_id: str, model_name: str) -> bool:
              artifacts = client.list_artifacts(run_id, path="")
              return any(a.path == model_name for a in artifacts)

          def file_uri_to_path(uri: str) -> Path | None:
              # Handle file:///... URIs (common for local artifact stores)
              if uri.startswith("file:"):
                  p = urlparse(uri).path
                  return Path(unquote(p))
              return None

          mlflow.set_tracking_uri(os.environ["MLFLOW_TRACKING_URI"])
          exp = mlflow.get_experiment_by_name(os.environ["MLFLOW_EXPERIMENT_NAME"])
          if exp is None:
              raise SystemExit("Experiment not found")

          client = MlflowClient()
          model_name = os.environ["MLFLOW_MODEL_NAME"]

          runs = client.search_runs(
              experiment_ids=[exp.experiment_id],
              filter_string="attributes.lifecycle_stage = 'active'",
              order_by=["attributes.start_time DESC"],
              max_results=500,
          )

          kept = 0
          removed = 0

          for r in runs:
              run_id = r.info.run_id
              if run_has_model_artifact(client, run_id, model_name):
                  kept += 1
                  continue

              # Mark deleted in tracking store
              client.delete_run(run_id)
              removed += 1

              # Best-effort delete artifact dir on disk (if local file URI)
              art_path = file_uri_to_path(r.info.artifact_uri)
              if art_path is not None and art_path.exists():
                  shutil.rmtree(art_path, ignore_errors=True)

          print(f"Pruned runs: removed={removed}, kept_with_model={kept}")
          if kept == 0:
              raise SystemExit(f"No MLflow runs with artifacts/{model_name} found after fallback.")
          PY

      - name: Execute notebook 02 (loads best model from MLflow)
        run: |
          set -euo pipefail
          jupyter nbconvert --execute --to notebook \
            --ExecutePreprocessor.timeout=1800 \
            --output-dir notebooks/_executed \
            notebooks/02_compare_runs.ipynb

      - name: Upload executed notebooks
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: executed-notebooks
          path: notebooks/_executed

      - name: Upload MLflow artifacts + DB
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: mlflow-tracking
          path: |
            mlflow.db
            mlflow-artifacts
