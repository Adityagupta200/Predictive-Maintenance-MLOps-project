name: Notebook Smoke

on:
  pull_request:
  push:
    branches: [master]
  workflow_dispatch:

jobs:
  notebook-smoke:
    runs-on: ubuntu-latest
    env:
      MLFLOW_TRACKING_URI: sqlite:///${{ github.workspace }}/mlflow.db
      MLFLOW_EXPERIMENT_NAME: predictive-maintenance-cmapss

      # Notebook 02 expects runs:/{run_id}/model
      MLFLOW_MODEL_NAME: model

      # Keep artifacts outside repo noise
      MLFLOW_ARTIFACT_ROOT: ${{ github.workspace }}/mlflow-artifacts

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps (incl. Jupyter + MLflow helpers)
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          make setup
          python -m pip install nbconvert jupyter ipykernel mlflow joblib

      - name: Clean MLflow state (CI only)
        run: |
          set -euo pipefail
          rm -f mlflow.db || true
          rm -rf "$MLFLOW_ARTIFACT_ROOT" || true
          rm -rf mlruns || true
          mkdir -p "$MLFLOW_ARTIFACT_ROOT"

      - name: Initialize MLflow experiment (before notebooks)
        run: |
          set -euo pipefail
          python - << 'PY'
          import os
          from pathlib import Path
          import mlflow

          mlflow.set_tracking_uri(os.environ["MLFLOW_TRACKING_URI"])

          exp_name = os.environ["MLFLOW_EXPERIMENT_NAME"]
          artifact_root = Path(os.environ["MLFLOW_ARTIFACT_ROOT"]).resolve()

          exp = mlflow.get_experiment_by_name(exp_name)
          if exp is None:
              exp_id = mlflow.create_experiment(exp_name, artifact_location=f"file:{artifact_root}")
              exp = mlflow.get_experiment(exp_id)
              print(f"Created experiment: {exp_name} ({exp.experiment_id}) artifact_location={exp.artifact_location}")
          else:
              print(f"Experiment exists: {exp_name} ({exp.experiment_id}) artifact_location={exp.artifact_location}")

          with open(os.environ["GITHUB_ENV"], "a", encoding="utf-8") as f:
              f.write(f"MLFLOW_EXPERIMENT_ID={exp.experiment_id}\n")
          PY

      - name: Prepare notebook inputs (generate artifacts)
        run: |
          set -euo pipefail
          mkdir -p artifacts/processed models

          if [ ! -f artifacts/processed/train.csv ] || [ ! -f artifacts/processed/val.csv ]; then
            if make -n preprocess >/dev/null 2>&1; then
              make preprocess
            elif make -n pipeline >/dev/null 2>&1; then
              make pipeline
            fi
          fi

          if [ ! -f models/best_model.joblib ]; then
            if make -n train >/dev/null 2>&1; then
              make train
            fi
          fi

          test -f artifacts/processed/train.csv || (echo "Missing artifacts/processed/train.csv" && exit 1)
          test -f artifacts/processed/val.csv || (echo "Missing artifacts/processed/val.csv" && exit 1)
          test -f models/best_model.joblib || (echo "Missing models/best_model.joblib" && exit 1)

          echo "TRAIN_CSV=${GITHUB_WORKSPACE}/artifacts/processed/train.csv" >> "$GITHUB_ENV"
          echo "VAL_CSV=${GITHUB_WORKSPACE}/artifacts/processed/val.csv" >> "$GITHUB_ENV"
          echo "MODEL_PATH=${GITHUB_WORKSPACE}/models/best_model.joblib" >> "$GITHUB_ENV"

      - name: Execute notebook 01 (logs MLflow runs)
        run: |
          set -euo pipefail
          mkdir -p notebooks/_executed
          jupyter nbconvert --execute --to notebook \
            --ExecutePreprocessor.timeout=1800 \
            --output-dir notebooks/_executed \
            notebooks/01_mlflow_experiments.ipynb

      - name: Ensure at least one run has a logged model (fallback if Notebook 01 didn't)
        run: |
          set -euo pipefail
          python - << 'PY'
          import os
          from pathlib import Path

          import joblib
          import mlflow
          import mlflow.sklearn
          from mlflow.tracking import MlflowClient

          def run_has_model_dir(client: MlflowClient, run_id: str, model_name: str) -> bool:
              # Strategy A: root listing contains "model" dir
              root = client.list_artifacts(run_id, path="")
              if any(a.path == model_name for a in root):
                  return True
              # Strategy B: listing inside "model" returns something (works even if root list omits dirs)
              inner = client.list_artifacts(run_id, path=model_name)
              return len(inner) > 0

          mlflow.set_tracking_uri(os.environ["MLFLOW_TRACKING_URI"])
          mlflow.set_experiment(os.environ["MLFLOW_EXPERIMENT_NAME"])

          client = MlflowClient()
          exp = mlflow.get_experiment_by_name(os.environ["MLFLOW_EXPERIMENT_NAME"])
          if exp is None:
              raise SystemExit("Experiment not found")

          model_name = os.environ["MLFLOW_MODEL_NAME"]

          runs = client.search_runs(
              experiment_ids=[exp.experiment_id],
              filter_string="",
              order_by=["start_time DESC"],
              max_results=200,
          )

          for r in runs:
              stage = getattr(r.info, "lifecycle_stage", "active")
              if stage == "deleted":
                  continue
              if run_has_model_dir(client, r.info.run_id, model_name):
                  print("Found existing run with model artifact directory; no fallback needed.")
                  raise SystemExit(0)

          model_path = Path(os.environ["MODEL_PATH"])
          if not model_path.exists():
              raise SystemExit(f"MODEL_PATH does not exist: {model_path}")

          model = joblib.load(model_path)

          with mlflow.start_run(run_name="ci-fallback-model") as run:
              mlflow.log_param("fallback_reason", "notebook_01_did_not_log_model")

              # Prefer new API if supported, but ALWAYS ensure it ends up under "model".
              logged_ok = False
              try:
                  mlflow.sklearn.log_model(model, name=model_name)
                  logged_ok = True
              except TypeError:
                  logged_ok = False

              if not logged_ok:
                  mlflow.sklearn.log_model(model, artifact_path=model_name)

              mlflow.log_metric("ci_fallback", 1.0)
              run_id = run.info.run_id

          if not run_has_model_dir(client, run_id, model_name):
              r = client.get_run(run_id)
              print(f"Fallback run artifact_uri={r.info.artifact_uri}")
              # Extra debugging: show what artifacts DO exist
              print("Root artifacts:", [a.path for a in client.list_artifacts(run_id, path="")])
              raise SystemExit(f"Fallback run did not produce '{model_name}' artifact as expected.")

          print(f"Created fallback run with model: {run_id}")
          PY

      - name: Prune runs missing model artifact (API-based; backend-agnostic)
        run: |
          set -euo pipefail
          python - << 'PY'
          import os
          import shutil
          from pathlib import Path
          from urllib.parse import urlparse, unquote

          import mlflow
          from mlflow.tracking import MlflowClient

          def run_has_model_dir(client: MlflowClient, run_id: str, model_name: str) -> bool:
              root = client.list_artifacts(run_id, path="")
              if any(a.path == model_name for a in root):
                  return True
              inner = client.list_artifacts(run_id, path=model_name)
              return len(inner) > 0

          def file_uri_to_path(uri: str) -> Path | None:
              if uri.startswith("file:"):
                  p = urlparse(uri).path
                  return Path(unquote(p))
              # If it's an absolute path (some local setups), allow cleanup
              p = Path(uri)
              return p if p.is_absolute() else None

          mlflow.set_tracking_uri(os.environ["MLFLOW_TRACKING_URI"])
          exp = mlflow.get_experiment_by_name(os.environ["MLFLOW_EXPERIMENT_NAME"])
          if exp is None:
              raise SystemExit("Experiment not found")

          client = MlflowClient()
          model_name = os.environ["MLFLOW_MODEL_NAME"]

          runs = client.search_runs(
              experiment_ids=[exp.experiment_id],
              filter_string="",
              order_by=["start_time DESC"],
              max_results=500,
          )

          kept = 0
          removed = 0

          for r in runs:
              stage = getattr(r.info, "lifecycle_stage", "active")
              if stage == "deleted":
                  continue

              if run_has_model_dir(client, r.info.run_id, model_name):
                  kept += 1
                  continue

              client.delete_run(r.info.run_id)
              removed += 1

              art_path = file_uri_to_path(r.info.artifact_uri)
              if art_path is not None and art_path.exists():
                  shutil.rmtree(art_path, ignore_errors=True)

          print(f"Pruned runs: removed={removed}, kept_with_model={kept}")
          if kept == 0:
              raise SystemExit(f"No MLflow runs with artifacts/{model_name} found after fallback.")
          PY

      - name: Execute notebook 02 (loads best model from MLflow)
        run: |
          set -euo pipefail
          jupyter nbconvert --execute --to notebook \
            --ExecutePreprocessor.timeout=1800 \
            --output-dir notebooks/_executed \
            notebooks/02_compare_runs.ipynb

      - name: Upload executed notebooks
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: executed-notebooks
          path: notebooks/_executed

      - name: Upload MLflow artifacts + DB
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: mlflow-tracking
          path: |
            mlflow.db
            mlflow-artifacts
            mlruns
