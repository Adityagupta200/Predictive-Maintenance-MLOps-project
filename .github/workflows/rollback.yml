name: rollback-production

on:
  workflow_dispatch:
    inputs:
      reason:
        description: "Reason for rollback (link to monitoring alert, etc.)"
        required: true
        type: string
      to_revision:
        description: "Optional: rollback to a specific revision number (leave empty to rollback to previous)"
        required: false
        type: string
      cluster_name:
        description: "EKS cluster name (optional override; else uses secret EKS_CLUSTER_NAME)"
        required: false
        type: string
      namespace:
        description: "Kubernetes namespace"
        required: true
        default: "default"
        type: string
      deployment_name:
        description: "Kubernetes deployment name"
        required: true
        default: "predictive-maintenance-api"
        type: string
      destroy_cluster_after_test:
        description: "Demo only: delete the EKS cluster after successful rollback verification"
        required: true
        default: "false"
        type: choice
        options: ["true", "false"]

  repository_dispatch:
    types: [model_degraded]

permissions:
  id-token: write
  contents: read

jobs:
  rollback:
    runs-on: ubuntu-latest
    concurrency:
      group: rollback-production
      cancel-in-progress: false

    env:
      AWS_REGION: ap-south-1

    steps:
      - uses: actions/checkout@v4

      - name: Resolve rollback inputs (manual vs automated)
        id: vars
        shell: bash
        run: |
          set -euo pipefail
          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            REASON="${{ inputs.reason }}"
            TO_REV="${{ inputs.to_revision }}"
            CLUSTER="${{ inputs.cluster_name }}"
            NS="${{ inputs.namespace }}"
            DEPLOY="${{ inputs.deployment_name }}"
            DESTROY="${{ inputs.destroy_cluster_after_test }}"
          else
            REASON="${{ github.event.client_payload.reason }}"
            TO_REV="${{ github.event.client_payload.to_revision }}"
            CLUSTER="${{ github.event.client_payload.cluster_name }}"
            NS="${{ github.event.client_payload.namespace }}"
            DEPLOY="${{ github.event.client_payload.deployment_name }}"
            DESTROY="false"
          fi

          if [ -z "${CLUSTER:-}" ]; then
            CLUSTER="${{ secrets.EKS_CLUSTER_NAME }}"
          fi

          test -n "${REASON}"
          test -n "${CLUSTER}"
          test -n "${NS}"
          test -n "${DEPLOY}"

          echo "reason=${REASON}" >> "$GITHUB_OUTPUT"
          echo "to_revision=${TO_REV:-}" >> "$GITHUB_OUTPUT"
          echo "cluster=${CLUSTER}" >> "$GITHUB_OUTPUT"
          echo "namespace=${NS}" >> "$GITHUB_OUTPUT"
          echo "deploy=${DEPLOY}" >> "$GITHUB_OUTPUT"
          echo "destroy=${DESTROY}" >> "$GITHUB_OUTPUT"

      - name: Configure AWS credentials (OIDC)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.EKS_DEPLOY }}
          aws-region: ${{ env.AWS_REGION }}
          audience: sts.amazonaws.com

      - name: IAM identity
        run: |
          set -euo pipefail
          aws sts get-caller-identity

      - name: Preflight: cluster exists and ACTIVE
        run: |
          set -euo pipefail
          aws eks describe-cluster \
            --name "${{ steps.vars.outputs.cluster }}" \
            --region "${AWS_REGION}" \
            --query "cluster.status" --output text
          aws eks wait cluster-active \
            --name "${{ steps.vars.outputs.cluster }}" \
            --region "${AWS_REGION}"
            
      - name: Install kubectl
        uses: azure/setup-kubectl@v4
        with:
          version: "v1.30.0"

      - name: Configure kubectl for EKS (with access role + auth check)
        env:
          EKS_KUBECTL_ROLE_ARN: ${{ secrets.EKS_KUBECTL_ROLE_ARN }}
        run: |
          set -euo pipefail

          ROLE_ARGS=""
          if [ -n "${EKS_KUBECTL_ROLE_ARN:-}" ]; then
            echo "Using EKS_KUBECTL_ROLE_ARN for kubectl auth"
            ROLE_ARGS="--role-arn ${EKS_KUBECTL_ROLE_ARN}"
          else
            echo "EKS_KUBECTL_ROLE_ARN not set; using current AWS identity for kubectl auth"
          fi

          aws eks update-kubeconfig \
            --name "${{ steps.vars.outputs.cluster }}" \
            --region "${AWS_REGION}" \
            ${ROLE_ARGS}

          # Hard auth check (fail early with actionable message)
          set +e
          kubectl -n "${{ steps.vars.outputs.namespace }}" get deploy "${{ steps.vars.outputs.deploy }}" -o wide >/tmp/kubectl_auth_check.log 2>&1
          rc=$?
          set -e

          if [ "$rc" -ne 0 ]; then
            echo "kubectl auth check failed. Output:"
            cat /tmp/kubectl_auth_check.log || true
            echo ""
            echo "Likely cause: the role used for kubectl is not authorized in the EKS cluster (aws-auth / access entries)."
            echo "Fix: set secret EKS_KUBECTL_ROLE_ARN to a role that is authorized in the cluster,"
            echo "or update cluster RBAC to authorize the GitHub Actions role."
            exit 1
          fi

      - name: Show current state (pre-rollback)
        run: |
          set -euo pipefail
          kubectl -n "${{ steps.vars.outputs.namespace }}" get deploy "${{ steps.vars.outputs.deploy }}" -o wide
          kubectl -n "${{ steps.vars.outputs.namespace }}" rollout history deployment/"${{ steps.vars.outputs.deploy }}" | tee rollout_history_before.txt

      - name: Roll back deployment
        run: |
          set -euo pipefail
          TO_REV="${{ steps.vars.outputs.to_revision }}"
          if [ -n "${TO_REV}" ]; then
            kubectl -n "${{ steps.vars.outputs.namespace }}" rollout undo deployment/"${{ steps.vars.outputs.deploy }}" --to-revision="${TO_REV}"
          else
            kubectl -n "${{ steps.vars.outputs.namespace }}" rollout undo deployment/"${{ steps.vars.outputs.deploy }}"
          fi
          kubectl -n "${{ steps.vars.outputs.namespace }}" rollout status deployment/"${{ steps.vars.outputs.deploy }}" --timeout=180s

      - name: Verify service health after rollback (port-forward + /health)
        run: |
          set -euo pipefail
          kubectl -n "${{ steps.vars.outputs.namespace }}" port-forward deployment/"${{ steps.vars.outputs.deploy }}" 18000:8000 >/tmp/pf.log 2>&1 &
          PF_PID=$!
          for i in $(seq 1 60); do
            if curl -fsS "http://127.0.0.1:18000/health" >/dev/null; then
              echo "Health OK after rollback"
              kill "${PF_PID}" || true
              exit 0
            fi
            sleep 2
          done
          echo "Health check failed after rollback; pf logs:"
          tail -n 200 /tmp/pf.log || true
          kill "${PF_PID}" || true
          exit 1

      - name: Show state (post-rollback)
        if: always()
        run: |
          set -euo pipefail
          kubectl -n "${{ steps.vars.outputs.namespace }}" get deploy "${{ steps.vars.outputs.deploy }}" -o wide
          kubectl -n "${{ steps.vars.outputs.namespace }}" rollout history deployment/"${{ steps.vars.outputs.deploy }}" | tee rollout_history_after.txt

      - name: Upload rollback artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: rollback-artifacts
          path: |
            rollout_history_before.txt
            rollout_history_after.txt

      - name: "Demo cleanup (optional): delete EKS cluster"
        if: steps.vars.outputs.destroy == 'true'
        run: |
          set -euo pipefail
          echo "Deleting cluster '${{ steps.vars.outputs.cluster }}' (demo cleanup)..."
          curl -sL "https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
          sudo mv /tmp/eksctl /usr/local/bin/eksctl
          eksctl delete cluster --name "${{ steps.vars.outputs.cluster }}" --region "${AWS_REGION}" --wait
