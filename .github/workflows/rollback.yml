name: rollback-production

on:
  workflow_dispatch:
    inputs:
      reason:
        description: "Reason for rollback (link to monitoring alert, etc.)"
        required: true
        type: string

      to_revision:
        description: "Optional: rollback to a specific revision number (leave empty to rollback to previous)"
        required: false
        type: string

      cluster_name:
        description: "EKS cluster name (optional override; else uses secret EKS_CLUSTER_NAME)"
        required: false
        type: string

      namespace:
        description: "Kubernetes namespace"
        required: true
        default: "default"
        type: string

      deployment_name:
        description: "Kubernetes deployment name"
        required: true
        default: "predictive-maintenance-api"
        type: string

      destroy_cluster_after_test:
        description: "Demo only: delete the EKS cluster after successful rollback verification (only pm-debug-* allowed)"
        required: true
        default: "false"
        type: choice
        options: ["true", "false"]

      bootstrap_eks_access:
        description: "If true, create/associate EKS access entry for the AWS role used by this workflow"
        required: true
        default: "true"
        type: choice
        options: ["true", "false"]

  repository_dispatch:
    types: [model_degraded]

permissions:
  id-token: write
  contents: read

jobs:
  rollback:
    runs-on: ubuntu-latest
    concurrency:
      group: rollback-production
      cancel-in-progress: false

    env:
      AWS_REGION: ap-south-1

    steps:
      - uses: actions/checkout@v4

      - name: Resolve rollback inputs (manual vs automated)
        id: vars
        shell: bash
        run: |
          set -euo pipefail

          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            REASON="${{ inputs.reason }}"
            TO_REV="${{ inputs.to_revision }}"
            CLUSTER="${{ inputs.cluster_name }}"
            NS="${{ inputs.namespace }}"
            DEPLOY="${{ inputs.deployment_name }}"
            DESTROY="${{ inputs.destroy_cluster_after_test }}"
            BOOTSTRAP="${{ inputs.bootstrap_eks_access }}"
          else
            REASON="${{ github.event.client_payload.reason }}"
            TO_REV="${{ github.event.client_payload.to_revision }}"
            CLUSTER="${{ github.event.client_payload.cluster_name }}"
            NS="${{ github.event.client_payload.namespace }}"
            DEPLOY="${{ github.event.client_payload.deployment_name }}"
            DESTROY="${{ github.event.client_payload.destroy_cluster_after_test || 'false' }}"
            BOOTSTRAP="${{ github.event.client_payload.bootstrap_eks_access || 'false' }}"
          fi

          if [ -z "${CLUSTER:-}" ]; then
            CLUSTER="${{ secrets.EKS_CLUSTER_NAME }}"
          fi

          test -n "${REASON}"
          test -n "${CLUSTER}"
          test -n "${NS}"
          test -n "${DEPLOY}"

          echo "reason=${REASON}" >> "$GITHUB_OUTPUT"
          echo "to_revision=${TO_REV:-}" >> "$GITHUB_OUTPUT"
          echo "cluster=${CLUSTER}" >> "$GITHUB_OUTPUT"
          echo "namespace=${NS}" >> "$GITHUB_OUTPUT"
          echo "deploy=${DEPLOY}" >> "$GITHUB_OUTPUT"
          echo "destroy=${DESTROY}" >> "$GITHUB_OUTPUT"
          echo "bootstrap=${BOOTSTRAP}" >> "$GITHUB_OUTPUT"

      - name: Normalize cluster name (EKS-safe)
        id: normalize
        shell: bash
        run: |
          set -euo pipefail

          raw="${{ steps.vars.outputs.cluster }}"

          # EKS cluster name must start with [A-Za-z0-9] and then [A-Za-z0-9-_]*
          name="$(echo "$raw" \
            | tr '[:upper:]' '[:lower:]' \
            | tr '/_' '--' \
            | tr -cd 'a-z0-9-')"

          if [ -z "$name" ]; then
            echo "Cluster name resolved to empty after sanitization. raw='$raw'"
            exit 1
          fi

          if ! echo "$name" | grep -Eq '^[a-z0-9]'; then
            name="pm-${name}"
          fi

          name="${name:0:80}"

          echo "cluster_normalized=$name" >> "$GITHUB_OUTPUT"
          echo "Using cluster (raw)        : $raw"
          echo "Using cluster (normalized) : $name"

      - name: Configure AWS credentials (OIDC)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.EKS_DEPLOY }}
          aws-region: ${{ env.AWS_REGION }}
          audience: sts.amazonaws.com

      - name: IAM identity
        run: |
          set -euo pipefail
          aws sts get-caller-identity

      - name: "Preflight: cluster exists and ACTIVE"
        run: |
          set -euo pipefail
          aws eks describe-cluster \
            --name "${{ steps.normalize.outputs.cluster_normalized }}" \
            --region "${AWS_REGION}" \
            --query "cluster.status" \
            --output text
          aws eks wait cluster-active \
            --name "${{ steps.normalize.outputs.cluster_normalized }}" \
            --region "${AWS_REGION}"

      - name: Install kubectl
        uses: azure/setup-kubectl@v4
        with:
          version: "v1.30.0"

      - name: Bootstrap EKS access entry (optional, fixes "provide credentials")
        if: steps.vars.outputs.bootstrap == 'true'
        env:
          EKS_KUBECTL_ROLE_ARN: ${{ secrets.EKS_KUBECTL_ROLE_ARN }}
          EKS_DEPLOY_ROLE_ARN: ${{ secrets.EKS_DEPLOY }}
        run: |
          set -euo pipefail
          PRINCIPAL="${EKS_KUBECTL_ROLE_ARN:-${EKS_DEPLOY_ROLE_ARN}}"
          test -n "${PRINCIPAL}"
          echo "Bootstrapping EKS access for principal: ${PRINCIPAL}"

          aws eks create-access-entry \
            --cluster-name "${{ steps.normalize.outputs.cluster_normalized }}" \
            --principal-arn "${PRINCIPAL}" \
            --region "${AWS_REGION}" || true

          aws eks associate-access-policy \
            --cluster-name "${{ steps.normalize.outputs.cluster_normalized }}" \
            --principal-arn "${PRINCIPAL}" \
            --policy-arn arn:aws:eks::aws:cluster-access-policy/AmazonEKSClusterAdminPolicy \
            --access-scope type=cluster \
            --region "${AWS_REGION}" || true

      - name: Configure kubectl for EKS (use current AWS identity)
        run: |
          set -euo pipefail
          aws eks update-kubeconfig --name "${{ steps.normalize.outputs.cluster_normalized }}" --region "${AWS_REGION}"

          set +e
          kubectl -n "${{ steps.vars.outputs.namespace }}" get deploy "${{ steps.vars.outputs.deploy }}" -o wide >/tmp/kubectl_auth_check.log 2>&1
          rc=$?
          set -e

          if [ "$rc" -ne 0 ]; then
            echo "kubectl auth check failed. Output:"
            cat /tmp/kubectl_auth_check.log || true
            echo ""
            echo "Fix options:"
            echo "1) Keep bootstrap_eks_access=true (recommended for demo)."
            echo "2) Or authorize the workflow role in EKS (Access Entry / aws-auth)."
            exit 1
          fi

      - name: Show current state (pre-rollback)
        run: |
          set -euo pipefail
          kubectl -n "${{ steps.vars.outputs.namespace }}" get deploy "${{ steps.vars.outputs.deploy }}" -o wide
          kubectl -n "${{ steps.vars.outputs.namespace }}" rollout history deployment/"${{ steps.vars.outputs.deploy }}" | tee rollout_history_before.txt

      - name: Roll back deployment
        run: |
          set -euo pipefail
          TO_REV="${{ steps.vars.outputs.to_revision }}"
          if [ -n "${TO_REV}" ]; then
            kubectl -n "${{ steps.vars.outputs.namespace }}" rollout undo deployment/"${{ steps.vars.outputs.deploy }}" --to-revision="${TO_REV}"
          else
            kubectl -n "${{ steps.vars.outputs.namespace }}" rollout undo deployment/"${{ steps.vars.outputs.deploy }}"
          fi
          kubectl -n "${{ steps.vars.outputs.namespace }}" rollout status deployment/"${{ steps.vars.outputs.deploy }}" --timeout=180s

      - name: Verify service health after rollback (port-forward + /health)
        run: |
          set -euo pipefail
          kubectl -n "${{ steps.vars.outputs.namespace }}" port-forward deployment/"${{ steps.vars.outputs.deploy }}" 18000:8000 >/tmp/pf.log 2>&1 &
          PF_PID=$!

          for i in $(seq 1 60); do
            if curl -fsS "http://127.0.0.1:18000/health" >/dev/null; then
              echo "Health OK after rollback"
              kill "${PF_PID}" || true
              exit 0
            fi
            sleep 2
          done

          echo "Health check failed after rollback; pf logs:"
          tail -n 200 /tmp/pf.log || true
          kill "${PF_PID}" || true
          exit 1

      - name: Show state (post-rollback)
        if: always()
        run: |
          set -euo pipefail
          kubectl -n "${{ steps.vars.outputs.namespace }}" get deploy "${{ steps.vars.outputs.deploy }}" -o wide
          kubectl -n "${{ steps.vars.outputs.namespace }}" rollout history deployment/"${{ steps.vars.outputs.deploy }}" | tee rollout_history_after.txt

      - name: Upload rollback artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: rollback-artifacts
          path: |
            rollout_history_before.txt
            rollout_history_after.txt

      - name: "Demo cleanup (optional): delete EKS cluster"
        # Only delete after successful rollback verification
        if: success() && steps.vars.outputs.destroy == 'true'
        shell: bash
        run: |
          set -euo pipefail
          CLUSTER="${{ steps.normalize.outputs.cluster_normalized }}"

          if [[ "$CLUSTER" != pm-debug-* ]]; then
            echo "Refusing to delete cluster '$CLUSTER' because it doesn't match pm-debug-* safety pattern."
            exit 1
          fi

          eksctl delete cluster --name "$CLUSTER" --region "${AWS_REGION}" --wait
