name: rollback-production

on:
  workflow_dispatch:
    inputs:
      reason:
        description: "Reason for rollback (link to monitoring alert, etc.)"
        required: true
        type: string

      to_revision:
        description: "Optional: rollback to a specific revision number (leave empty to rollback to previous)"
        required: false
        type: string

      cluster_name:
        description: "EKS cluster name (optional override; else uses secret EKS_CLUSTER_NAME)"
        required: false
        type: string

      namespace:
        description: "Kubernetes namespace"
        required: true
        default: "default"
        type: string

      deployment_name:
        description: "Kubernetes deployment name"
        required: true
        default: "predictive-maintenance-api"
        type: string

      destroy_cluster_after_test:
        description: "Demo only: delete the EKS cluster after successful rollback verification (only pm-debug-* allowed)"
        required: true
        default: "false"
        type: choice
        options: ["true", "false"]

      bootstrap_eks_access:
        description: "If true, create/associate EKS access entry for the AWS role used by this workflow"
        required: true
        default: "true"
        type: choice
        options: ["true", "false"]

  repository_dispatch:
    types: [model_degraded]

permissions:
  id-token: write
  contents: read

jobs:
  rollback:
    runs-on: ubuntu-latest
    concurrency:
      group: rollback-production
      cancel-in-progress: false

    env:
      AWS_REGION: ap-south-1

    steps:
      - uses: actions/checkout@v4

      - name: Resolve rollback inputs (manual vs automated)
        id: vars
        shell: bash
        run: |
          set -euo pipefail

          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            REASON="${{ inputs.reason }}"
            TO_REV="${{ inputs.to_revision }}"
            CLUSTER="${{ inputs.cluster_name }}"
            NS="${{ inputs.namespace }}"
            DEPLOY="${{ inputs.deployment_name }}"
            DESTROY="${{ inputs.destroy_cluster_after_test }}"
            BOOTSTRAP="${{ inputs.bootstrap_eks_access }}"
          else
            REASON="${{ github.event.client_payload.reason }}"
            TO_REV="${{ github.event.client_payload.to_revision }}"
            CLUSTER="${{ github.event.client_payload.cluster_name }}"
            NS="${{ github.event.client_payload.namespace }}"
            DEPLOY="${{ github.event.client_payload.deployment_name }}"
            DESTROY="${{ github.event.client_payload.destroy_cluster_after_test || 'false' }}"
            BOOTSTRAP="${{ github.event.client_payload.bootstrap_eks_access || 'false' }}"
          fi

          if [ -z "${CLUSTER:-}" ]; then
            CLUSTER="${{ secrets.EKS_CLUSTER_NAME }}"
          fi

          test -n "${REASON}"
          test -n "${CLUSTER}"
          test -n "${NS}"
          test -n "${DEPLOY}"

          echo "reason=${REASON}" >> "$GITHUB_OUTPUT"
          echo "to_revision=${TO_REV:-}" >> "$GITHUB_OUTPUT"
          echo "cluster=${CLUSTER}" >> "$GITHUB_OUTPUT"
          echo "namespace=${NS}" >> "$GITHUB_OUTPUT"
          echo "deploy=${DEPLOY}" >> "$GITHUB_OUTPUT"
          echo "destroy=${DESTROY}" >> "$GITHUB_OUTPUT"
          echo "bootstrap=${BOOTSTRAP}" >> "$GITHUB_OUTPUT"

      - name: Normalize cluster name (EKS-safe)
        id: normalize
        shell: bash
        run: |
          set -euo pipefail
          raw="${{ steps.vars.outputs.cluster }}"

          name="$(echo "$raw" \
            | tr '[:upper:]' '[:lower:]' \
            | tr '/_' '--' \
            | tr -cd 'a-z0-9-')"

          if [ -z "$name" ]; then
            echo "Cluster name resolved to empty after sanitization. raw='$raw'"
            exit 1
          fi

          if ! echo "$name" | grep -Eq '^[a-z0-9]'; then
            name="pm-${name}"
          fi

          name="${name:0:80}"
          echo "cluster_normalized=$name" >> "$GITHUB_OUTPUT"
          echo "Using cluster (raw)        : $raw"
          echo "Using cluster (normalized) : $name"

      - name: Configure AWS credentials (OIDC)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.EKS_DEPLOY }}
          aws-region: ${{ env.AWS_REGION }}
          audience: sts.amazonaws.com

      - name: IAM identity
        run: |
          set -euo pipefail
          aws sts get-caller-identity

      - name: "Preflight: cluster exists and ACTIVE"
        run: |
          set -euo pipefail
          aws eks describe-cluster \
            --name "${{ steps.normalize.outputs.cluster_normalized }}" \
            --region "${AWS_REGION}" \
            --query "cluster.status" \
            --output text
          aws eks wait cluster-active \
            --name "${{ steps.normalize.outputs.cluster_normalized }}" \
            --region "${AWS_REGION}"

      - name: Install kubectl
        uses: azure/setup-kubectl@v4
        with:
          version: "v1.30.0"

      - name: Bootstrap EKS access entry (optional, fixes "provide credentials")
        if: steps.vars.outputs.bootstrap == 'true'
        env:
          EKS_KUBECTL_ROLE_ARN: ${{ secrets.EKS_KUBECTL_ROLE_ARN }}
          EKS_DEPLOY_ROLE_ARN: ${{ secrets.EKS_DEPLOY }}
        run: |
          set -euo pipefail
          PRINCIPAL="${EKS_KUBECTL_ROLE_ARN:-${EKS_DEPLOY_ROLE_ARN}}"
          test -n "${PRINCIPAL}"
          echo "Bootstrapping EKS access for principal: ${PRINCIPAL}"

          aws eks create-access-entry \
            --cluster-name "${{ steps.normalize.outputs.cluster_normalized }}" \
            --principal-arn "${PRINCIPAL}" \
            --region "${AWS_REGION}" || true

          aws eks associate-access-policy \
            --cluster-name "${{ steps.normalize.outputs.cluster_normalized }}" \
            --principal-arn "${PRINCIPAL}" \
            --policy-arn arn:aws:eks::aws:cluster-access-policy/AmazonEKSClusterAdminPolicy \
            --access-scope type=cluster \
            --region "${AWS_REGION}" || true

      - name: Configure kubectl for EKS (use current AWS identity)
        run: |
          set -euo pipefail
          aws eks update-kubeconfig --name "${{ steps.normalize.outputs.cluster_normalized }}" --region "${AWS_REGION}"

          set +e
          kubectl -n "${{ steps.vars.outputs.namespace }}" get deploy "${{ steps.vars.outputs.deploy }}" -o wide >/tmp/kubectl_auth_check.log 2>&1
          rc=$?
          set -e

          if [ "$rc" -ne 0 ]; then
            echo "kubectl auth check failed. Output:"
            cat /tmp/kubectl_auth_check.log || true
            exit 1
          fi

      - name: Show current state (pre-rollback)
        run: |
          set -euo pipefail
          kubectl -n "${{ steps.vars.outputs.namespace }}" get deploy "${{ steps.vars.outputs.deploy }}" -o wide
          kubectl -n "${{ steps.vars.outputs.namespace }}" rollout history deployment/"${{ steps.vars.outputs.deploy }}" | tee rollout_history_before.txt

      - name: Roll back deployment (do not fail on rollout-status timeout; health check decides)
        id: rollback_apply
        run: |
          set -euo pipefail

          TO_REV="${{ steps.vars.outputs.to_revision }}"
          if [ -n "${TO_REV}" ]; then
            kubectl -n "${{ steps.vars.outputs.namespace }}" rollout undo deployment/"${{ steps.vars.outputs.deploy }}" --to-revision="${TO_REV}"
          else
            kubectl -n "${{ steps.vars.outputs.namespace }}" rollout undo deployment/"${{ steps.vars.outputs.deploy }}"
          fi

          # Rollout status can time out if old replicas take time to terminate.
          # Treat this as non-fatal; the real gate is /health after rollback.
          set +e
          kubectl -n "${{ steps.vars.outputs.namespace }}" rollout status deployment/"${{ steps.vars.outputs.deploy }}" --timeout=600s
          rc=$?
          set -e

          echo "rollout_status_rc=$rc" >> "$GITHUB_OUTPUT"
          if [ "$rc" -ne 0 ]; then
            echo "WARNING: rollout status timed out; continuing to diagnostics + health verification."
          fi

      - name: Diagnostics (only if rollout status timed out)
        if: steps.rollback_apply.outputs.rollout_status_rc != '0'
        run: |
          set -euo pipefail
          ns="${{ steps.vars.outputs.namespace }}"
          dep="${{ steps.vars.outputs.deploy }}"

          echo "---- deploy ----"
          kubectl -n "$ns" get deploy "$dep" -o wide || true
          kubectl -n "$ns" describe deploy "$dep" || true

          echo "---- rs ----"
          kubectl -n "$ns" get rs -o wide || true

          echo "---- pods ----"
          kubectl -n "$ns" get pods -l app="$dep" -o wide || true
          kubectl -n "$ns" get pods -o wide || true

          echo "---- events (tail) ----"
          kubectl -n "$ns" get events --sort-by=.lastTimestamp | tail -n 80 || true

      - name: Verify service health after rollback (port-forward + /health)
        id: health
        run: |
          set -euo pipefail

          ns="${{ steps.vars.outputs.namespace }}"
          dep="${{ steps.vars.outputs.deploy }}"

          rm -f /tmp/pf.log || true

          kubectl -n "$ns" port-forward deployment/"$dep" 18000:8000 --address 127.0.0.1 >/tmp/pf.log 2>&1 &
          PF_PID=$!

          # Wait up to 30s for port-forward to become ready (or fail fast if it dies)
          ready="false"
          for i in $(seq 1 30); do
            if ! kill -0 "$PF_PID" 2>/dev/null; then
              echo "port-forward exited early; logs:"
              tail -n 200 /tmp/pf.log || true
              break
            fi
            if grep -q "Forwarding from 127.0.0.1:18000" /tmp/pf.log; then
              ready="true"
              break
            fi
            sleep 1
          done

          if [ "$ready" != "true" ]; then
            echo "port-forward never became ready; logs:"
            tail -n 200 /tmp/pf.log || true
            exit 1
          fi

          ok="false"
          for i in $(seq 1 60); do
            if curl -fsS "http://127.0.0.1:18000/health" >/dev/null; then
              ok="true"
              break
            fi
            sleep 2
          done

          kill "$PF_PID" || true

          echo "ok=${ok}" >> "$GITHUB_OUTPUT"
          if [ "$ok" != "true" ]; then
            echo "Health check failed; port-forward logs:"
            tail -n 200 /tmp/pf.log || true
          fi


      - name: Fail workflow if rollback verification failed
        if: steps.health.outputs.ok != 'true'
        run: |
          echo "Rollback verification failed (service /health not OK)."
          exit 1

      - name: Show state (post-rollback)
        if: always()
        run: |
          set -euo pipefail
          kubectl -n "${{ steps.vars.outputs.namespace }}" get deploy "${{ steps.vars.outputs.deploy }}" -o wide
          kubectl -n "${{ steps.vars.outputs.namespace }}" rollout history deployment/"${{ steps.vars.outputs.deploy }}" | tee rollout_history_after.txt

      - name: Upload rollback artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: rollback-artifacts
          path: |
            rollout_history_before.txt
            rollout_history_after.txt

      - name: Install eksctl (needed for cluster deletion)
        if: success() && steps.vars.outputs.destroy == 'true'
        run: |
          set -euo pipefail
          curl -sL "https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
          sudo mv /tmp/eksctl /usr/local/bin/eksctl
          eksctl version

      - name: "Demo cleanup (optional): delete EKS cluster"
        # Only delete AFTER successful rollback verification
        if: success() && steps.vars.outputs.destroy == 'true'
        shell: bash
        run: |
          set -euo pipefail
          CLUSTER="${{ steps.normalize.outputs.cluster_normalized }}"

          if [[ "$CLUSTER" != pm-debug-* ]]; then
            echo "Refusing to delete cluster '$CLUSTER' because it doesn't match pm-debug-* safety pattern."
            exit 1
          fi

          eksctl delete cluster --name "$CLUSTER" --region "${AWS_REGION}" --wait
