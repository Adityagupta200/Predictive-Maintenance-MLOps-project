name: ci-cd

permissions:
  id-token: write
  contents: read

on:
  pull_request:
    branches: [master]
  workflow_dispatch:
    inputs:
      run_ephemeral_eks_drill:
        description: "Create/reuse EKS, deploy, run post-deploy gate, rollback on failure, then destroy (on success)."
        required: true
        default: "false"
        type: choice
        options:
          - "true"
          - "false"
      destroy_on_failure:
        description: "If true, destroys EKS even when the drill fails. Default keeps cluster for debugging/reuse."
        required: true
        default: "false"
        type: choice
        options:
          - "true"
          - "false"

# Prevent two runs from fighting over the same reused debug cluster on the same branch.
concurrency:
  group: pm-eks-debug-${{ github.ref }}
  cancel-in-progress: false

env:
  PYTHON_VERSION: "3.11"

  # MLflow & gating defaults (point 1 in PDF)
  MLFLOW_TRACKING_URI: "file:./mlruns"
  MLFLOW_EXPERIMENT_NAME: "cmapss_rul_xgb_optuna"
  METRIC_NAME: "r2"
  METRIC_MODE: "max"
  METRIC_THRESHOLD: "0.90"
  METRICS_PATH: "artifacts/metrics/metrics.json"

jobs:
  test-train-validate:
    runs-on: ubuntu-latest
    env:
      MLFLOW_TRACKING_URI: "file:./mlruns"
      MLFLOW_EXPERIMENT_NAME: "cmapss_rul_xgb_optuna"

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          if [ -f requirements-api.txt ]; then pip install -r requirements-api.txt; fi
          pip install pytest

      - name: Add src to PYTHONPATH
        run: echo "PYTHONPATH=$PWD/src" >> "$GITHUB_ENV"

      - name: Debug repo tree
        run: |
          pwd
          ls -la
          ls -la src || true
          ls -la src/api || true

      - name: Run unit tests (PyTest)
        run: pytest -q

      - name: Run DVC pipeline (train + validate)
        run: dvc repro

      - name: Verify model artifact exists
        run: |
          ls -la models || true
          test -f models/best_model.joblib

      - name: Upload model artifact for deploy
        uses: actions/upload-artifact@v4
        with:
          name: model-artifact
          path: models/best_model.joblib
          if-no-files-found: error

      - name: Upload validation data for post-deploy checks
        uses: actions/upload-artifact@v4
        with:
          name: post-deploy-data
          path: artifacts/processed/val.csv
          if-no-files-found: error

      - name: Enforce quality gate
        env:
          MLFLOW_TRACKING_URI: "file:./mlruns"
          MLFLOW_EXPERIMENT_NAME: "cmapss_rul_xgb_optuna"
          METRIC_NAME: "${{ env.METRIC_NAME }}"
          METRIC_MODE: "${{ env.METRIC_MODE }}"
          METRIC_THRESHOLD: "${{ env.METRIC_THRESHOLD }}"
          METRICS_PATH: "${{ env.METRICS_PATH }}"
        run: python .github/scripts/check_accuracy_gate.py

  ephemeral-eks-drill:
    needs: test-train-validate
    if: >
      needs.test-train-validate.result == 'success' &&
      github.event_name == 'workflow_dispatch' &&
      inputs.run_ephemeral_eks_drill == 'true'
    runs-on: ubuntu-latest
    timeout-minutes: 90

    permissions:
      id-token: write
      contents: read

    env:
      AWS_REGION: ap-south-1

      # Stable cluster naming for reuse across reruns (branch-scoped).
      # Will be normalized in a step to remove illegal chars.
      CLUSTER_NAME: pm-debug-${{ github.repository_id }}-${{ github.ref_name }}

      K8S_NAMESPACE: default
      DEPLOYMENT_NAME: predictive-maintenance-api
      IMAGE_TAG: ${{ github.sha }}
      IMAGE_REPO: ${{ secrets.DOCKER_IMAGE_REPO }}

      # Gate thresholds
      P95_MS_MAX: "200"
      POST_DEPLOY_R2_MIN: "0.90"

      # Gate files
      VAL_CSV: "artifacts/processed/val.csv"
      VAL_CSV_ALIGNED: "artifacts/processed/val_for_api.csv"
      POST_DEPLOY_OUT: "artifacts/post_deploy_gate.json"
      N_REQUESTS: "50"

      # In-cluster gate base URL (ClusterIP service created by this workflow)
      IN_CLUSTER_BASE_URL: "http://predictive-maintenance-svc:8000"

      DESTROY_ON_FAILURE: ${{ inputs.destroy_on_failure }}

    defaults:
      run:
        shell: bash

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Normalize cluster name (stable + EKS-safe)
        id: normalize_cluster
        run: |
          set -euo pipefail
          raw="${CLUSTER_NAME}"
          name="$(echo "$raw" | tr '[:upper:]' '[:lower:]' | tr '/_' '--' | tr -cd 'a-z0-9-')"
          if [ -z "$name" ]; then
            name="pm-debug"
          fi
          if ! echo "$name" | grep -Eq '^[a-z]'; then
            name="pm-${name}"
          fi
          name="${name:0:80}"
          echo "CLUSTER_NAME=$name" >> "$GITHUB_ENV"
          echo "cluster_name=$name" >> "$GITHUB_OUTPUT"
          echo "Using CLUSTER_NAME=$name"

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Download model artifact
        uses: actions/download-artifact@v4
        with:
          name: model-artifact
          path: .github_artifacts

      - name: Download validation data (val.csv)
        uses: actions/download-artifact@v4
        with:
          name: post-deploy-data
          path: .post_deploy_artifacts

      - name: Place model + val.csv into build/test context
        run: |
          set -euo pipefail
          mkdir -p models artifacts/processed

          if [ -f .github_artifacts/models/best_model.joblib ]; then
            cp .github_artifacts/models/best_model.joblib models/best_model.joblib
          elif [ -f .github_artifacts/best_model.joblib ]; then
            cp .github_artifacts/best_model.joblib models/best_model.joblib
          else
            echo "Downloaded model artifact contents:"
            ls -R .github_artifacts
            exit 1
          fi
          test -f models/best_model.joblib

          if [ -f .post_deploy_artifacts/val.csv ]; then
            cp .post_deploy_artifacts/val.csv artifacts/processed/val.csv
          elif [ -f .post_deploy_artifacts/artifacts/processed/val.csv ]; then
            cp .post_deploy_artifacts/artifacts/processed/val.csv artifacts/processed/val.csv
          else
            echo "Downloaded post-deploy artifacts contents:"
            ls -R .post_deploy_artifacts
            exit 1
          fi
          test -f artifacts/processed/val.csv

      - name: Log in to Docker registry
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKER_USERNAME }}
          password: ${{ secrets.DOCKER_PASSWORD }}

      - name: Build and push image
        id: build
        run: |
          set -euo pipefail
          IMAGE="${IMAGE_REPO}:${IMAGE_TAG}"
          docker build -f infra/docker/DockerFile -t "${IMAGE}" .
          docker push "${IMAGE}"
          echo "image=${IMAGE}" >> "$GITHUB_OUTPUT"

      - name: Configure AWS credentials (OIDC)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.EKS_DEPLOY }}
          aws-region: ${{ env.AWS_REGION }}
          audience: sts.amazonaws.com

      - name: Get Role Identity
        run: aws sts get-caller-identity

      - name: Install eksctl
        run: |
          set -euo pipefail
          curl -sL "https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
          sudo mv /tmp/eksctl /usr/local/bin/eksctl
          eksctl version

      - name: Install envsubst (gettext-base)
        run: |
          set -euo pipefail
          sudo apt-get update
          sudo apt-get install -y gettext-base

      - name: Check if EKS cluster exists (reuse)
        id: check_cluster
        run: |
          set -euo pipefail
          if aws eks describe-cluster --name "${CLUSTER_NAME}" --region "${AWS_REGION}" >/dev/null 2>&1; then
            echo "exists=true" >> "$GITHUB_OUTPUT"
            status="$(aws eks describe-cluster --name "${CLUSTER_NAME}" --region "${AWS_REGION}" --query 'cluster.status' --output text)"
            echo "Cluster exists. status=${status}"
            if [ "${status}" != "ACTIVE" ]; then
              echo "Waiting for cluster to become ACTIVE..."
              aws eks wait cluster-active --name "${CLUSTER_NAME}" --region "${AWS_REGION}"
            fi
          else
            echo "exists=false" >> "$GITHUB_OUTPUT"
            echo "Cluster does not exist; will create."
          fi

      - name: Render eksctl cluster config (create only)
        if: steps.check_cluster.outputs.exists != 'true'
        run: |
          set -euo pipefail
          export CLUSTER_NAME AWS_REGION
          envsubst < .github/eks/cluster.yaml > /tmp/cluster.yaml
          echo "Rendered cluster config:"
          cat /tmp/cluster.yaml

      - name: Create EKS cluster (only if missing)
        id: create_cluster
        if: steps.check_cluster.outputs.exists != 'true'
        run: |
          set -euo pipefail
          eksctl create cluster -f /tmp/cluster.yaml

      - name: Install kubectl
        uses: azure/setup-kubectl@v4
        with:
          version: "v1.30.0"

      - name: Configure kubectl for EKS
        id: kubeconfig
        run: |
          set -euo pipefail
          aws eks update-kubeconfig --name "${CLUSTER_NAME}" --region "${AWS_REGION}"

      - name: Deploy (ephemeral drill: Deployment only)
        run: |
          set -euo pipefail
          kubectl apply -f infra/k8s/deployment.yaml -n "${K8S_NAMESPACE}"
          kubectl set image deployment/"${DEPLOYMENT_NAME}" api="${{ steps.build.outputs.image }}" -n "${K8S_NAMESPACE}"
          kubectl rollout status deployment/"${DEPLOYMENT_NAME}" -n "${K8S_NAMESPACE}" --timeout=600s

      - name: Create/Update ClusterIP service for in-cluster testing
        run: |
          set -euo pipefail
          cat <<'YAML' | kubectl apply -n "${K8S_NAMESPACE}" -f -
          apiVersion: v1
          kind: Service
          metadata:
            name: predictive-maintenance-svc
          spec:
            type: ClusterIP
            selector:
              app: predictive-maintenance-api
            ports:
              - name: http
                protocol: TCP
                port: 8000
                targetPort: 8000
          YAML

      - name: Wait for API pods ready
        run: |
          set -euo pipefail
          kubectl wait -n "${K8S_NAMESPACE}" --for=condition=Ready pod -l app="${DEPLOYMENT_NAME}" --timeout=300s
          kubectl -n "${K8S_NAMESPACE}" get pods -l app="${DEPLOYMENT_NAME}" -o wide

      - name: Build model-aligned val.csv for API (fix feature-count mismatch)
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          # install pinned API/runtime deps so joblib can load XGBoost-based models reliably
          pip install -r requirements-api.txt joblib

          python - <<'PY'
          import os
          from pathlib import Path

          import numpy as np
          import pandas as pd
          import joblib

          src_csv = Path(os.environ["VAL_CSV"])
          out_csv = Path(os.environ["VAL_CSV_ALIGNED"])
          model_path = Path("models/best_model.joblib")
          out_csv.parent.mkdir(parents=True, exist_ok=True)

          df = pd.read_csv(src_csv)
          model = joblib.load(model_path)

          def find_attr(obj, attr, visited=None, depth=0, max_depth=6):
            if visited is None:
              visited = set()
            oid = id(obj)
            if oid in visited:
              return None
            visited.add(oid)

            if hasattr(obj, attr):
              val = getattr(obj, attr)
              if val is not None:
                return val

            if depth >= max_depth:
              return None

            if hasattr(obj, "named_steps"):
              for step in obj.named_steps.values():
                v = find_attr(step, attr, visited, depth + 1, max_depth)
                if v is not None:
                  return v

            if hasattr(obj, "steps"):
              try:
                for _, step in obj.steps:
                  v = find_attr(step, attr, visited, depth + 1, max_depth)
                  if v is not None:
                    return v
              except Exception:
                pass

            for child_name in ("best_estimator_", "estimator", "regressor", "classifier", "model"):
              if hasattr(obj, child_name):
                v = find_attr(getattr(obj, child_name), attr, visited, depth + 1, max_depth)
                if v is not None:
                  return v

            return None

          feature_names = find_attr(model, "feature_names_in_")
          n_features = find_attr(model, "n_features_in_")

          if feature_names is not None:
            feature_names = list(feature_names)
            n_features = len(feature_names)

          X0 = df.drop(columns=["RUL"], errors="ignore")
          drop_like = [c for c in X0.columns if str(c).lower().startswith("unnamed")]
          if drop_like:
            X0 = X0.drop(columns=drop_like, errors="ignore")

          X0 = X0.apply(pd.to_numeric, errors="coerce")
          X0 = X0.dropna(axis=1, how="all")

          if n_features is None:
            raise SystemExit(
              "Could not infer n_features from model artifact. "
              "Ensure the saved model exposes n_features_in_ or feature_names_in_."
            )

          if feature_names is not None:
            X = df.reindex(columns=feature_names).apply(pd.to_numeric, errors="coerce")
          else:
            X = X0.copy()
            if X.shape[1] > n_features:
              X = X.iloc[:, :n_features]
            elif X.shape[1] < n_features:
              for i in range(n_features - X.shape[1]):
                X[f"_pad_{i}"] = 0.0

          X_valid = X.dropna(axis=0, how="any")
          keep_rul = "RUL" in df.columns

          if len(X_valid) == 0:
            X_valid = X.fillna(0.0).head(1)
            keep_rul = False

          if keep_rul:
            y = pd.to_numeric(df.loc[X_valid.index, "RUL"], errors="coerce")
            if y.isna().any():
              keep_rul = False

          if keep_rul:
            out = pd.concat([X_valid.reset_index(drop=True), y.reset_index(drop=True).rename("RUL")], axis=1)
          else:
            out = X_valid.reset_index(drop=True)

          out.to_csv(out_csv, index=False)
          print(f"Wrote {out_csv} with cols={len(out.columns)} (features={n_features}, keep_rul={keep_rul})")
          PY

      - name: Post-deploy gate (in-cluster: smoke + p95 + optional R2)
        id: post_deploy_gate
        run: |
          set -euo pipefail

          # Ensure clean reruns on reused cluster
          kubectl delete pod post-deploy-gate -n "${K8S_NAMESPACE}" --ignore-not-found=true || true

          # Create a short-lived pod to run the gate from inside the cluster network
          kubectl run post-deploy-gate \
            -n "${K8S_NAMESPACE}" \
            --image=python:3.11-slim \
            --restart=Never \
            --command -- sleep 3600

          kubectl wait -n "${K8S_NAMESPACE}" --for=condition=Ready pod/post-deploy-gate --timeout=180s

          kubectl exec -n "${K8S_NAMESPACE}" post-deploy-gate -- bash -lc "mkdir -p /work/.github/scripts /work/artifacts/processed"

          # Copy pinned deps + gate script + aligned val csv into the pod
          kubectl cp requirements-api.txt "${K8S_NAMESPACE}/post-deploy-gate:/work/requirements-api.txt"
          kubectl cp .github/scripts/post_deploy_gate.py "${K8S_NAMESPACE}/post-deploy-gate:/work/.github/scripts/post_deploy_gate.py"
          kubectl cp "${VAL_CSV_ALIGNED}" "${K8S_NAMESPACE}/post-deploy-gate:/work/artifacts/processed/val_for_api.csv"

          # Install deps inside the pod and run the gate
          kubectl exec -n "${K8S_NAMESPACE}" post-deploy-gate -- bash -lc "python -m pip install --upgrade pip && pip install -r /work/requirements-api.txt requests"

          kubectl exec -n "${K8S_NAMESPACE}" post-deploy-gate -- bash -lc "\
            python /work/.github/scripts/post_deploy_gate.py \
              --base-url '${IN_CLUSTER_BASE_URL}' \
              --val-csv /work/artifacts/processed/val_for_api.csv \
              --n-requests '${N_REQUESTS}' \
              --p95-ms-max '${P95_MS_MAX}' \
              --r2-min '${POST_DEPLOY_R2_MIN}' \
              --out /work/post_deploy_gate.json \
          "

          # Copy report back to runner for artifact upload
          mkdir -p artifacts
          kubectl cp "${K8S_NAMESPACE}/post-deploy-gate:/work/post_deploy_gate.json" "${POST_DEPLOY_OUT}"

      - name: Cleanup gate pod (always)
        if: always()
        run: |
          set -euo pipefail
          kubectl delete pod post-deploy-gate -n "${K8S_NAMESPACE}" --ignore-not-found=true || true

      - name: Rollback on gate failure (in-cluster)
        if: failure() && steps.kubeconfig.outcome == 'success'
        continue-on-error: true
        run: |
          set -euo pipefail
          kubectl rollout undo deployment/"${DEPLOYMENT_NAME}" -n "${K8S_NAMESPACE}" || true
          kubectl rollout status deployment/"${DEPLOYMENT_NAME}" -n "${K8S_NAMESPACE}" --timeout=180s || true

      - name: Upload post-deploy report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: post-deploy-report
          path: artifacts/post_deploy_gate.json
          if-no-files-found: warn

      - name: Destroy EKS resources (success or override)
        if: success() || env.DESTROY_ON_FAILURE == 'true'
        run: |
          set -euo pipefail

          # Delete in-cluster test service (safe even if missing)
          if [ -f "${HOME}/.kube/config" ]; then
            kubectl delete svc predictive-maintenance-svc -n "${K8S_NAMESPACE}" --ignore-not-found || true
            kubectl delete -f infra/k8s/deployment.yaml -n "${K8S_NAMESPACE}" --ignore-not-found || true
          fi

          if aws eks describe-cluster --name "${CLUSTER_NAME}" --region "${AWS_REGION}" >/dev/null 2>&1; then
            eksctl delete cluster --name "${CLUSTER_NAME}" --region "${AWS_REGION}" --wait
          else
            echo "Cluster ${CLUSTER_NAME} does not exist; skipping eksctl delete."
          fi

      - name: Keep cluster for debugging/reuse (failure)
        if: failure() && env.DESTROY_ON_FAILURE != 'true'
        run: |
          set -euo pipefail
          echo "Drill failed and cluster is being kept for reuse on next run."
          echo "IMPORTANT: You are billed while it runs."
          echo ""
          echo "To destroy manually when done debugging:"
          echo "  eksctl delete cluster --name \"${CLUSTER_NAME}\" --region \"${AWS_REGION}\" --wait"
