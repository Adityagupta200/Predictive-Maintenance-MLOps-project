name: rollback-production

on:
  workflow_dispatch:
    inputs:
      reason:
        description: "Reason for rollback (link to monitoring alert, etc.)"
        required: true
        type: string
      to_revision:
        description: "Optional: rollback to a specific revision number (leave empty to auto-select previous non-placeholder image)"
        required: false
        type: string
      cluster_name:
        description: "EKS cluster name (optional override; else uses secret EKS_CLUSTER_NAME)"
        required: false
        type: string
      namespace:
        description: "Kubernetes namespace"
        required: true
        default: "default"
        type: string
      deployment_name:
        description: "Kubernetes deployment name"
        required: true
        default: "predictive-maintenance-api"
        type: string
      destroy_cluster_after_test:
        description: "Demo only: delete the EKS cluster after successful rollback verification (only pm--* allowed)"
        required: true
        default: "false"
        type: choice
        options: ["true", "false"]
      bootstrap_eks_access:
        description: "If true, create/associate EKS access entry for the AWS role used by this workflow"
        required: true
        default: "true"
        type: choice
        options: ["true", "false"]

  repository_dispatch:
    types: [model_degraded]

permissions:
  id-token: write
  contents: read

jobs:
  rollback:
    runs-on: ubuntu-latest
    concurrency:
      group: rollback-production
      cancel-in-progress: false

    env:
      AWS_REGION: ap-south-1

    steps:
      - uses: actions/checkout@v4

      - name: Resolve rollback inputs (manual vs automated)
        id: vars
        shell: bash
        run: |
          set -euo pipefail

          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            REASON="${{ inputs.reason }}"
            TO_REV="${{ inputs.to_revision }}"
            CLUSTER="${{ inputs.cluster_name }}"
            NS="${{ inputs.namespace }}"
            DEPLOY="${{ inputs.deployment_name }}"
            DESTROY="${{ inputs.destroy_cluster_after_test }}"
            BOOTSTRAP="${{ inputs.bootstrap_eks_access }}"
          else
            REASON="${{ github.event.client_payload.reason }}"
            TO_REV="${{ github.event.client_payload.to_revision }}"
            CLUSTER="${{ github.event.client_payload.cluster_name }}"
            NS="${{ github.event.client_payload.namespace }}"
            DEPLOY="${{ github.event.client_payload.deployment_name }}"
            DESTROY="${{ github.event.client_payload.destroy_cluster_after_test || 'false' }}"
            BOOTSTRAP="${{ github.event.client_payload.bootstrap_eks_access || 'false' }}"
          fi

          if [ -z "${CLUSTER:-}" ]; then
            CLUSTER="${{ secrets.EKS_CLUSTER_NAME }}"
          fi

          test -n "${REASON}"
          test -n "${CLUSTER}"
          test -n "${NS}"
          test -n "${DEPLOY}"

          echo "reason=${REASON}" >> "$GITHUB_OUTPUT"
          echo "to_revision=${TO_REV:-}" >> "$GITHUB_OUTPUT"
          echo "cluster=${CLUSTER}" >> "$GITHUB_OUTPUT"
          echo "namespace=${NS}" >> "$GITHUB_OUTPUT"
          echo "deploy=${DEPLOY}" >> "$GITHUB_OUTPUT"
          echo "destroy=${DESTROY}" >> "$GITHUB_OUTPUT"
          echo "bootstrap=${BOOTSTRAP}" >> "$GITHUB_OUTPUT"

      - name: Normalize cluster name (EKS-safe)
        id: normalize
        shell: bash
        run: |
          set -euo pipefail
          raw="${{ steps.vars.outputs.cluster }}"
          name="$(echo "$raw" \
            | tr '[:upper:]' '[:lower:]' \
            | tr '/_' '--' \
            | tr -cd 'a-z0-9-')"

          if [ -z "$name" ]; then
            echo "Cluster name resolved to empty after sanitization. raw='$raw'"
            exit 1
          fi

          if ! echo "$name" | grep -Eq '^[a-z0-9]'; then
            name="pm-${name}"
          fi

          name="${name:0:80}"
          echo "cluster_normalized=$name" >> "$GITHUB_OUTPUT"
          echo "Using cluster (raw)        : $raw"
          echo "Using cluster (normalized) : $name"

      - name: Configure AWS credentials (OIDC)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.EKS_DEPLOY }}
          aws-region: ${{ env.AWS_REGION }}
          audience: sts.amazonaws.com

      - name: IAM identity
        run: |
          set -euo pipefail
          aws sts get-caller-identity

      - name: "Preflight: cluster exists and ACTIVE"
        run: |
          set -euo pipefail
          aws eks describe-cluster \
            --name "${{ steps.normalize.outputs.cluster_normalized }}" \
            --region "${AWS_REGION}" \
            --query "cluster.status" \
            --output text
          aws eks wait cluster-active \
            --name "${{ steps.normalize.outputs.cluster_normalized }}" \
            --region "${AWS_REGION}"

      - name: Install kubectl
        uses: azure/setup-kubectl@v4
        with:
          version: "v1.30.0"

      - name: Install jq (needed to pick correct rollback image)
        run: |
          set -euo pipefail
          sudo apt-get update
          sudo apt-get install -y jq

      - name: Bootstrap EKS access entry (optional, fixes "provide credentials")
        if: steps.vars.outputs.bootstrap == 'true'
        env:
          EKS_KUBECTL_ROLE_ARN: ${{ secrets.EKS_KUBECTL_ROLE_ARN }}
          EKS_DEPLOY_ROLE_ARN: ${{ secrets.EKS_DEPLOY }}
        run: |
          set -euo pipefail
          PRINCIPAL="${EKS_KUBECTL_ROLE_ARN:-${EKS_DEPLOY_ROLE_ARN}}"
          test -n "${PRINCIPAL}"
          echo "Bootstrapping EKS access for principal: ${PRINCIPAL}"

          aws eks create-access-entry \
            --cluster-name "${{ steps.normalize.outputs.cluster_normalized }}" \
            --principal-arn "${PRINCIPAL}" \
            --region "${AWS_REGION}" || true

          aws eks associate-access-policy \
            --cluster-name "${{ steps.normalize.outputs.cluster_normalized }}" \
            --principal-arn "${PRINCIPAL}" \
            --policy-arn arn:aws:eks::aws:cluster-access-policy/AmazonEKSClusterAdminPolicy \
            --access-scope type=cluster \
            --region "${AWS_REGION}" || true

      - name: Configure kubectl for EKS (use current AWS identity)
        run: |
          set -euo pipefail
          aws eks update-kubeconfig --name "${{ steps.normalize.outputs.cluster_normalized }}" --region "${AWS_REGION}"

          kubectl -n "${{ steps.vars.outputs.namespace }}" get deploy "${{ steps.vars.outputs.deploy }}" -o wide >/dev/null

      - name: Show current state (pre-rollback)
        run: |
          set -euo pipefail
          kubectl -n "${{ steps.vars.outputs.namespace }}" get deploy "${{ steps.vars.outputs.deploy }}" -o wide
          kubectl -n "${{ steps.vars.outputs.namespace }}" rollout history deployment/"${{ steps.vars.outputs.deploy }}" | tee rollout_history_before.txt

      - name: Roll back (never pick placeholder image)
        id: rollback_apply
        run: |
          set -euo pipefail
          ns="${{ steps.vars.outputs.namespace }}"
          dep="${{ steps.vars.outputs.deploy }}"
          to_rev="${{ steps.vars.outputs.to_revision }}"

          # If user explicitly requested a revision, do that.
          if [ -n "${to_rev}" ]; then
            echo "Rolling back to explicit revision: ${to_rev}"
            kubectl -n "$ns" rollout undo deployment/"$dep" --to-revision="${to_rev}"
          else
            # Auto-select rollback image:
            # pick the most recent ReplicaSet image that is NOT ':placeholder' and NOT the current image.
            cur_img="$(kubectl -n "$ns" get deploy "$dep" -o json | jq -r '.spec.template.spec.containers[] | select(.name=="api") | .image')"
            cur_rev="$(kubectl -n "$ns" get deploy "$dep" -o json | jq -r '.metadata.annotations["deployment.kubernetes.io/revision"] // "0"' )"

            echo "Current image: $cur_img"
            echo "Current revision: $cur_rev"

            rs_json="$(kubectl -n "$ns" get rs -l app=predictive-maintenance-api -o json)"
            candidate="$(echo "$rs_json" | jq -r --arg cur "$cur_img" --argjson currev "$cur_rev" '
              [
                .items[]
                | {
                    rev: (.metadata.annotations["deployment.kubernetes.io/revision"]|tonumber),
                    img: (.spec.template.spec.containers[] | select(.name=="api") | .image)
                  }
              ]
              | map(select(.rev < $currev and (.img|contains(":placeholder")|not) and .img != $cur))
              | sort_by(.rev)
              | last
              | .img // empty
            ')"

            if [ -z "$candidate" ]; then
              echo "ERROR: Could not find a previous non-placeholder image to roll back to."
              echo "ReplicaSets seen:"
              echo "$rs_json" | jq -r '.items[] | "\(.metadata.name) rev=\(.metadata.annotations["deployment.kubernetes.io/revision"]) img=\(.spec.template.spec.containers[]|select(.name=="api")|.image)"'
              exit 1
            fi

            echo "Auto-selected rollback image: $candidate"
            kubectl -n "$ns" set image deployment/"$dep" api="$candidate"
          fi

          # Rollout status can still be slow; do not hard-fail here.
          set +e
          kubectl -n "$ns" rollout status deployment/"$dep" --timeout=600s
          rc=$?
          set -e
          echo "rollout_status_rc=$rc" >> "$GITHUB_OUTPUT"
          if [ "$rc" -ne 0 ]; then
            echo "WARNING: rollout status timed out; continuing to diagnostics + health verification."
          fi

      - name: Diagnostics (only if rollout status timed out)
        if: steps.rollback_apply.outputs.rollout_status_rc != '0'
        run: |
          set -euo pipefail
          ns="${{ steps.vars.outputs.namespace }}"
          dep="${{ steps.vars.outputs.deploy }}"

          echo "---- deploy ----"
          kubectl -n "$ns" get deploy "$dep" -o wide || true
          kubectl -n "$ns" describe deploy "$dep" || true

          echo "---- rs ----"
          kubectl -n "$ns" get rs -o wide || true

          echo "---- pods ----"
          kubectl -n "$ns" get pods -o wide || true

          echo "---- events (tail) ----"
          kubectl -n "$ns" get events --sort-by=.lastTimestamp | tail -n 120 || true

      - name: Verify service health after rollback (port-forward + /health)
        id: health
        run: |
          set -euo pipefail
          ns="${{ steps.vars.outputs.namespace }}"
          dep="${{ steps.vars.outputs.deploy }}"

          rm -f /tmp/pf.log || true
          kubectl -n "$ns" port-forward deployment/"$dep" 18000:8000 --address 127.0.0.1 >/tmp/pf.log 2>&1 &
          PF_PID=$!

          # Wait for port-forward readiness or fail fast if it exits
          ready="false"
          for i in $(seq 1 30); do
            if ! kill -0 "$PF_PID" 2>/dev/null; then
              echo "port-forward exited early; logs:"
              tail -n 200 /tmp/pf.log || true
              break
            fi
            if grep -q "Forwarding from 127.0.0.1:18000" /tmp/pf.log; then
              ready="true"
              break
            fi
            sleep 1
          done

          if [ "$ready" != "true" ]; then
            echo "port-forward never became ready; logs:"
            tail -n 200 /tmp/pf.log || true
            exit 1
          fi

          ok="false"
          for i in $(seq 1 60); do
            if curl -fsS "http://127.0.0.1:18000/health" >/dev/null; then
              ok="true"
              break
            fi
            sleep 2
          done

          kill "$PF_PID" || true
          echo "ok=${ok}" >> "$GITHUB_OUTPUT"

          if [ "$ok" != "true" ]; then
            echo "Health check failed; port-forward logs:"
            tail -n 200 /tmp/pf.log || true
          fi

      - name: Fail workflow if rollback verification failed
        if: steps.health.outputs.ok != 'true'
        run: |
          echo "Rollback verification failed (service /health not OK)."
          exit 1

      - name: Show state (post-rollback)
        if: always()
        run: |
          set -euo pipefail
          kubectl -n "${{ steps.vars.outputs.namespace }}" get deploy "${{ steps.vars.outputs.deploy }}" -o wide
          kubectl -n "${{ steps.vars.outputs.namespace }}" rollout history deployment/"${{ steps.vars.outputs.deploy }}" | tee rollout_history_after.txt

      - name: Upload rollback artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: rollback-artifacts
          path: |
            rollout_history_before.txt
            rollout_history_after.txt

      - name: Install eksctl (needed for cluster deletion)
        if: success() && steps.vars.outputs.destroy == 'true'
        run: |
          set -euo pipefail
          curl -sL "https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
          sudo mv /tmp/eksctl /usr/local/bin/eksctl
          eksctl version

      - name: "Demo cleanup (optional): delete EKS cluster"
        if: >
          success() &&
          steps.vars.outputs.destroy == 'true' &&
          steps.health.outputs.ok == 'true' &&
          steps.rollback_apply.outputs.rollout_status_rc == '0'
        shell: bash
        run: |
          set -euo pipefail
          CLUSTER="${{ steps.normalize.outputs.cluster_normalized }}"
          if [[ "$CLUSTER" != pm-* ]]; then
            echo "Refusing to delete cluster '$CLUSTER' because it doesn't match pm-* safety pattern."
            exit 1
          fi
          eksctl delete cluster --name "$CLUSTER" --region "${AWS_REGION}" --wait
